#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "asyncio",
#     "watchdog>=3.0.0",
#     "python-dateutil>=2.8.0",
# ]
# ///

"""
ABOUTME: Real-time monitoring system for Claude Code hook events.
ABOUTME: Provides immediate feedback and alerts based on log analysis patterns.
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass
from collections import defaultdict, deque
import time

from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

from log_analyzer import LogAnalyzer, LogEvent, PatternDetector


@dataclass
class Alert:
    """Represents an alert generated by the monitoring system."""
    alert_type: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    message: str
    timestamp: str
    context: Dict[str, Any]
    session_id: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'alert_type': self.alert_type,
            'severity': self.severity,
            'message': self.message,
            'timestamp': self.timestamp,
            'context': self.context,
            'session_id': self.session_id
        }


class AlertManager:
    """Manages alert generation, deduplication, and notification."""
    
    def __init__(self, alert_cooldown: int = 300):  # 5 minutes default cooldown
        self.alert_cooldown = alert_cooldown
        self.recent_alerts = deque(maxlen=100)
        self.alert_counts = defaultdict(int)
        self.last_alert_time = defaultdict(float)
        
    def should_alert(self, alert: Alert) -> bool:
        """Determine if an alert should be sent based on deduplication rules."""
        alert_key = f"{alert.alert_type}:{alert.severity}"
        current_time = time.time()
        
        # Check cooldown period
        if current_time - self.last_alert_time[alert_key] < self.alert_cooldown:
            return False
        
        # Allow critical alerts to bypass some restrictions
        if alert.severity == 'critical':
            return True
        
        # Check if we've seen too many similar alerts recently
        recent_similar = sum(1 for a in self.recent_alerts 
                           if a.alert_type == alert.alert_type 
                           and (current_time - time.mktime(datetime.fromisoformat(a.timestamp.replace('Z', '+00:00')).timetuple())) < 3600)
        
        if recent_similar >= 5:  # Max 5 similar alerts per hour
            return False
        
        return True
    
    def add_alert(self, alert: Alert) -> bool:
        """Add an alert if it passes deduplication checks."""
        if self.should_alert(alert):
            self.recent_alerts.append(alert)
            alert_key = f"{alert.alert_type}:{alert.severity}"
            self.alert_counts[alert_key] += 1
            self.last_alert_time[alert_key] = time.time()
            return True
        return False
    
    def get_recent_alerts(self, minutes: int = 60) -> List[Alert]:
        """Get alerts from the last N minutes."""
        cutoff_time = time.time() - (minutes * 60)
        return [
            alert for alert in self.recent_alerts
            if time.mktime(datetime.fromisoformat(alert.timestamp.replace('Z', '+00:00')).timetuple()) > cutoff_time
        ]


class RealTimeAnalyzer:
    """Real-time analyzer that processes events as they occur."""
    
    def __init__(self):
        self.pattern_detector = PatternDetector()
        self.alert_manager = AlertManager()
        self.performance_baselines = defaultdict(list)
        self.session_states = defaultdict(dict)
        self.metrics_window = deque(maxlen=1000)  # Keep last 1000 events for baseline calculation
        
    def analyze_event(self, event: LogEvent) -> List[Alert]:
        """Analyze a single event and generate alerts if needed."""
        self.metrics_window.append(event)
        alerts = []
        
        # Update session state
        self._update_session_state(event)
        
        # Run pattern detection
        pattern_analysis = self.pattern_detector.analyze_event(event)
        
        # Generate alerts based on patterns
        alerts.extend(self._generate_performance_alerts(event, pattern_analysis))
        alerts.extend(self._generate_error_alerts(event, pattern_analysis))
        alerts.extend(self._generate_anomaly_alerts(event, pattern_analysis))
        alerts.extend(self._generate_success_alerts(event, pattern_analysis))
        
        # Filter through alert manager
        filtered_alerts = []
        for alert in alerts:
            if self.alert_manager.add_alert(alert):
                filtered_alerts.append(alert)
        
        return filtered_alerts
    
    def _update_session_state(self, event: LogEvent):
        """Update session state tracking."""
        session_id = event.session_id
        session_state = self.session_states[session_id]
        
        # Track session metrics
        if 'start_time' not in session_state:
            session_state['start_time'] = event.datetime
        
        session_state['last_activity'] = event.datetime
        session_state['event_count'] = session_state.get('event_count', 0) + 1
        
        # Track tool usage
        if event.event_type == 'tool_use':
            tool_name = event.data.get('tool_name', 'unknown')
            tool_usage = session_state.setdefault('tool_usage', defaultdict(int))
            tool_usage[tool_name] += 1
        
        # Track errors
        if event.data.get('error'):
            session_state['error_count'] = session_state.get('error_count', 0) + 1
    
    def _generate_performance_alerts(self, event: LogEvent, analysis: Dict) -> List[Alert]:
        """Generate alerts for performance issues."""
        alerts = []
        
        for issue in analysis.get('performance_issues', []):
            if issue['type'] == 'slow_tool_execution':
                severity = issue.get('severity', 'medium')
                
                alert = Alert(
                    alert_type='slow_tool_execution',
                    severity=severity,
                    message=f"Tool '{issue['tool_name']}' executed slowly: {issue['execution_time']:.2f}s "
                           f"(baseline: {issue['baseline']:.2f}s)",
                    timestamp=event.timestamp,
                    context={
                        'tool_name': issue['tool_name'],
                        'execution_time': issue['execution_time'],
                        'baseline': issue['baseline'],
                        'slowdown_factor': issue['execution_time'] / issue['baseline']
                    },
                    session_id=event.session_id
                )
                alerts.append(alert)
        
        return alerts
    
    def _generate_error_alerts(self, event: LogEvent, analysis: Dict) -> List[Alert]:
        """Generate alerts for error patterns."""
        alerts = []
        
        for error in analysis.get('error_patterns', []):
            if error['type'] == 'recurring_error':
                alert = Alert(
                    alert_type='recurring_error',
                    severity='high',
                    message=f"Recurring error detected in '{error['tool_name']}': "
                           f"{error['occurrence_count']} occurrences",
                    timestamp=event.timestamp,
                    context={
                        'tool_name': error['tool_name'],
                        'error_message': error['error_message'][:200],
                        'occurrence_count': error['occurrence_count']
                    },
                    session_id=event.session_id
                )
                alerts.append(alert)
        
        return alerts
    
    def _generate_anomaly_alerts(self, event: LogEvent, analysis: Dict) -> List[Alert]:
        """Generate alerts for anomalous patterns."""
        alerts = []
        
        for anomaly in analysis.get('anomalies', []):
            if anomaly['type'] == 'unusual_session_duration':
                alert = Alert(
                    alert_type='session_anomaly',
                    severity='medium',
                    message=f"Unusual session duration detected: {anomaly['session_duration']:.2f}s "
                           f"(typical: {anomaly['baseline_median']:.2f}s)",
                    timestamp=event.timestamp,
                    context={
                        'session_duration': anomaly['session_duration'],
                        'baseline_median': anomaly['baseline_median']
                    },
                    session_id=event.session_id
                )
                alerts.append(alert)
        
        return alerts
    
    def _generate_success_alerts(self, event: LogEvent, analysis: Dict) -> List[Alert]:
        """Generate alerts for success patterns (positive feedback)."""
        alerts = []
        
        for success in analysis.get('success_indicators', []):
            if success['type'] == 'high_success_agent':
                alert = Alert(
                    alert_type='success_pattern',
                    severity='low',  # Positive alert
                    message=f"Agent '{success['agent_name']}' showing high success rate: "
                           f"{success['success_rate']:.1%}",
                    timestamp=event.timestamp,
                    context={
                        'agent_name': success['agent_name'],
                        'success_rate': success['success_rate'],
                        'sample_size': success['sample_size']
                    },
                    session_id=event.session_id
                )
                alerts.append(alert)
        
        return alerts
    
    def get_session_summary(self, session_id: str) -> Dict[str, Any]:
        """Get a summary of a session's activity."""
        session_state = self.session_states.get(session_id, {})
        
        if not session_state:
            return {'error': 'Session not found'}
        
        duration = None
        if 'start_time' in session_state and 'last_activity' in session_state:
            duration = (session_state['last_activity'] - session_state['start_time']).total_seconds()
        
        return {
            'session_id': session_id,
            'duration_seconds': duration,
            'event_count': session_state.get('event_count', 0),
            'tool_usage': dict(session_state.get('tool_usage', {})),
            'error_count': session_state.get('error_count', 0),
            'start_time': session_state.get('start_time', '').isoformat() if session_state.get('start_time') else None,
            'last_activity': session_state.get('last_activity', '').isoformat() if session_state.get('last_activity') else None
        }


class LogFileWatcher(FileSystemEventHandler):
    """Watches log files for changes and triggers real-time analysis."""
    
    def __init__(self, analyzer: RealTimeAnalyzer, notification_callback: Optional[Callable] = None):
        self.analyzer = analyzer
        self.notification_callback = notification_callback
        self.monitored_files = {}  # file_path -> last_modified_time
        
    def on_modified(self, event):
        """Handle file modification events."""
        if event.is_directory:
            return
        
        file_path = Path(event.src_path)
        
        # Only process JSON log files
        if not file_path.name.endswith('.json'):
            return
        
        # Check if this is a file we should monitor
        if not self._should_monitor_file(file_path):
            return
        
        # Process new log entries
        asyncio.create_task(self._process_log_file_changes(file_path))
    
    def _should_monitor_file(self, file_path: Path) -> bool:
        """Determine if a file should be monitored."""
        monitored_patterns = [
            'post_tool_use.json',
            'user_prompt_submit.json',
            'pre_tool_use.json',
            'session_start.json'
        ]
        
        return any(pattern in file_path.name for pattern in monitored_patterns)
    
    async def _process_log_file_changes(self, file_path: Path):
        """Process changes to a log file."""
        try:
            # Read the file
            with open(file_path) as f:
                log_data = json.load(f)
            
            # Get last processed position for this file
            last_processed = self.monitored_files.get(str(file_path), 0)
            
            # Process new entries
            if isinstance(log_data, list) and len(log_data) > last_processed:
                new_entries = log_data[last_processed:]
                
                for entry in new_entries:
                    if isinstance(entry, dict):
                        event = LogEvent(
                            event_type=entry.get('event_type', 'unknown'),
                            session_id=entry.get('session_id', 'unknown'),
                            timestamp=entry.get('timestamp', datetime.now().isoformat()),
                            data=entry,
                            metrics=entry.get('metrics', {})
                        )
                        
                        # Analyze the event
                        alerts = self.analyzer.analyze_event(event)
                        
                        # Send notifications if callback provided
                        if self.notification_callback and alerts:
                            await self.notification_callback(alerts)
                
                # Update last processed position
                self.monitored_files[str(file_path)] = len(log_data)
                
        except (json.JSONDecodeError, FileNotFoundError, Exception) as e:
            logging.error(f"Error processing log file {file_path}: {e}")


class RealTimeMonitor:
    """Main real-time monitoring system."""
    
    def __init__(self, log_directory: str = "logs", global_logs_path: str = "/Users/joshuaoliphant/Library/CloudStorage/Dropbox/python_workspace/digital_garden/logs"):
        self.log_directory = Path(log_directory)
        self.global_logs_path = Path(global_logs_path)
        self.analyzer = RealTimeAnalyzer()
        self.observer = Observer()
        self.notification_callbacks = []
        
        # Ensure log directory exists
        self.log_directory.mkdir(exist_ok=True)
        
    def add_notification_callback(self, callback: Callable[[List[Alert]], None]):
        """Add a callback function to be called when alerts are generated."""
        self.notification_callbacks.append(callback)
    
    async def _notify_callbacks(self, alerts: List[Alert]):
        """Notify all registered callbacks about alerts."""
        for callback in self.notification_callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(alerts)
                else:
                    callback(alerts)
            except Exception as e:
                logging.error(f"Error in notification callback: {e}")
    
    def start_monitoring(self):
        """Start the real-time monitoring system."""
        # Set up file watcher
        event_handler = LogFileWatcher(self.analyzer, self._notify_callbacks)
        
        # Monitor local logs directory
        self.observer.schedule(event_handler, str(self.log_directory), recursive=True)
        
        # Monitor global logs if it exists and is different
        if self.global_logs_path.exists() and self.global_logs_path != self.log_directory:
            self.observer.schedule(event_handler, str(self.global_logs_path), recursive=True)
        
        # Start the observer
        self.observer.start()
        logging.info("Real-time monitoring started")
    
    def stop_monitoring(self):
        """Stop the real-time monitoring system."""
        self.observer.stop()
        self.observer.join()
        logging.info("Real-time monitoring stopped")
    
    def get_recent_alerts(self, minutes: int = 60) -> List[Alert]:
        """Get recent alerts."""
        return self.analyzer.alert_manager.get_recent_alerts(minutes)
    
    def get_session_summary(self, session_id: str) -> Dict[str, Any]:
        """Get a summary of a session."""
        return self.analyzer.get_session_summary(session_id)
    
    def get_system_health(self) -> Dict[str, Any]:
        """Get overall system health metrics."""
        recent_alerts = self.get_recent_alerts(60)
        
        alert_counts = defaultdict(int)
        for alert in recent_alerts:
            alert_counts[alert.severity] += 1
        
        return {
            'timestamp': datetime.now().isoformat(),
            'monitored_sessions': len(self.analyzer.session_states),
            'recent_alerts': {
                'total': len(recent_alerts),
                'by_severity': dict(alert_counts)
            },
            'active_monitoring': self.observer.is_alive() if hasattr(self.observer, 'is_alive') else True
        }


async def console_alert_callback(alerts: List[Alert]):
    """Simple console notification callback for testing."""
    for alert in alerts:
        severity_symbols = {
            'low': '‚úì',
            'medium': '‚ö†',
            'high': '‚ùå',
            'critical': 'üö®'
        }
        
        symbol = severity_symbols.get(alert.severity, '?')
        print(f"{symbol} [{alert.severity.upper()}] {alert.alert_type}: {alert.message}")


async def main():
    """Main entry point for command-line usage."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Real-time Claude Code log monitor')
    parser.add_argument('--log-dir', default='logs', help='Log directory to monitor')
    parser.add_argument('--console-output', action='store_true', 
                       help='Output alerts to console')
    parser.add_argument('--health-check', action='store_true',
                       help='Show system health and exit')
    
    args = parser.parse_args()
    
    monitor = RealTimeMonitor(args.log_dir)
    
    if args.health_check:
        health = monitor.get_system_health()
        print(json.dumps(health, indent=2))
        return
    
    if args.console_output:
        monitor.add_notification_callback(console_alert_callback)
    
    # Start monitoring
    monitor.start_monitoring()
    
    try:
        print("Real-time monitoring active. Press Ctrl+C to stop.")
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        print("\nStopping monitor...")
    finally:
        monitor.stop_monitoring()


if __name__ == '__main__':
    asyncio.run(main())